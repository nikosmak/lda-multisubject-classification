{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import ast\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "from openpyxl import load_workbook\n",
    "from scipy.spatial import distance\n",
    "import spacy\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer, SnowballStemmer, PorterStemmer\n",
    "from greek_stemmer import GreekStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import textacy.extract.basics\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from modern_greek_accentuation.accentuation import *\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models.wrappers.ldamallet import LdaMallet\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from scipy.signal import find_peaks\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import json\n",
    "from sklearn.metrics._classification import precision_recall_fscore_support\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "''' Constants and Global Variables'''\n",
    "\n",
    "#LDA Training model constants\n",
    "\n",
    "#documents to exclude from dictionary\n",
    "#words to exclude if they appear in less than number of documents\n",
    "not_less_than = 1\n",
    "#words to exclude if they appear in more than percent of documents\n",
    "not_more_than = 0.75\n",
    "\n",
    "#LDA Optimal model iterations\n",
    "OPTIMIMUM_NUMBER_OF_TOPICS_ENABLED = True\n",
    "limit = 70\n",
    "start = 25\n",
    "step = 2\n",
    "\n",
    "'''the prominence is used for finding the optimum coherence,\n",
    "Lower prominence is for more sensitive fluctuations of coherence values'''\n",
    "#peaks_prominence = 0.01\n",
    "peaks_prominence = 0.005\n",
    "# peaks_prominence = 0.008\n",
    "\n",
    "''' Attention this is neccessary to be installed and set for the LDA algorith to function.\n",
    "Check the manual for instructions'''\n",
    "os.environ['MALLET_HOME'] = '/usr/local/mallet'\n",
    "mallet_path = \"/usr/local/mallet/bin/mallet\"\n",
    "\n",
    "\n",
    "#prepocessing constants\n",
    "nlp = None\n",
    "LANGUAGE = 'english'\n",
    "STEMMING_ENABLED = True\n",
    "MIN_WORDS_PER_DOC = 2\n",
    "SPACE = \" \"\n",
    "\n",
    "\n",
    "\n",
    "'''Themes distribution variables and constants'''\n",
    "ALL_CATEGORIES = True \n",
    "\n",
    "'''Mapping between the standard documents and the springer category names.'''\n",
    "CATEGORIES_MAPPING = {\n",
    "    'Computer science': 'COMPUTER SCIENCE',\n",
    "    'Engineering': 'TECH SCIENCES AND ENGINEERING',\n",
    "    'Mathematics': 'MATHEMATICS',\n",
    "    'Medicine' : 'HEALTH SCIENCES',\n",
    "    'Physics':'PHYSICAL SCIENCES'\n",
    "    }\n",
    "\n",
    "\n",
    "def name_by_categories_no(name, all_categories_enabled):\n",
    "    return name if all_categories_enabled else f'{name}_5_categories'\n",
    "\n",
    "\n",
    "springer_dictionary_path = os.path.join(os.path.abspath(''), f'{name_by_categories_no(\"springer.dictionary\",ALL_CATEGORIES)}')\n",
    "springer_model_path = os.path.join(os.path.abspath(''), f'{name_by_categories_no(\"springer_lda.model\",ALL_CATEGORIES)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Utility functions '''\n",
    "\n",
    "''' Flattens a list of lists to a single list'''\n",
    "def flatten(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "'''Normalizes the rows of dataframe'''\n",
    "def normalize_df(df):\n",
    "    df = df.apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "    return df.div(df.sum(axis=1), axis=0)\n",
    "\n",
    "def normalize_rounded_df(df):\n",
    "    return normalize_df(df).round(3)\n",
    "\n",
    "def normalize_columns_df(df):\n",
    "    df = df.apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "    return df.div(df.sum(axis=0), axis=1)\n",
    "\n",
    "\n",
    "def normalize_columns_rounded_df(df):\n",
    "    return normalize_columns_df(df).round(3)\n",
    "\n",
    "def accumulated_frequency_of(items_to_check, items):\n",
    "    accumulated_frequency = 0\n",
    "    common_items = set(items_to_check).intersection(items)\n",
    "    for item_to_check in common_items:\n",
    "        accumulated_frequency += frequency_of(item_to_check, items)\n",
    "    return accumulated_frequency\n",
    "\n",
    "def frequency_of(item_to_check, items):\n",
    "    return items.count(item_to_check)\n",
    "\n",
    "def string_to_list(list_str):\n",
    "    return ast.literal_eval(list_str)\n",
    "\n",
    "def save_to_file(filename, data):\n",
    "    if(type(data) is dict):\n",
    "        w = csv.writer(open(filename + '.csv', \"w\"), delimiter=';')\n",
    "        # loop over dictionary keys and values\n",
    "        for key, val in data.items():\n",
    "            # write every key and value to file\n",
    "            w.writerow([key, val])\n",
    "    else:\n",
    "        df = pd.DataFrame(data)\n",
    "        # df.to_pickle(filename + '.pkl')\n",
    "        \n",
    "        df.to_csv(filename + '.csv', sep=';')\n",
    "\n",
    "def load_csv_to_dict(filename):\n",
    "    with open(f'{filename}.csv', mode='r') as infile:\n",
    "        reader = csv.reader(infile, delimiter=';')\n",
    "        loaded_dict = {rows[0]:string_to_list(rows[1]) for rows in reader}\n",
    "        return loaded_dict\n",
    "\n",
    "def isNaN(string):\n",
    "    return string != string\n",
    "\n",
    "def replace_file_char_with_char(path, replace_from, replace_to):\n",
    "    # read input file\n",
    "    fin = open(path, \"rt\")\n",
    "    # read file contents to string\n",
    "    data = fin.read()\n",
    "    # replace all occurrences of the required string\n",
    "    data = data.replace(replace_from, replace_to)\n",
    "    # close the input file\n",
    "    fin.close()\n",
    "    # open the input file in write mode\n",
    "    fin = open(path, \"wt\")\n",
    "    # overrite the input file with the resulting data\n",
    "    fin.write(data)\n",
    "    # close the file\n",
    "    fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc09304",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Preprocessing functions '''\n",
    "def preprocess(docs, language, with_stemming):\n",
    "    tic = time.perf_counter()\n",
    "    print(f\"Preprocessing of {1 if isinstance(docs, str) else len(docs) } {language} documents starting...\")\n",
    "    processed_docs = normalize_docs(docs=docs, lang=language, with_stemming=with_stemming, minimum_words_per_doc=MIN_WORDS_PER_DOC)\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Normalisation of of {1 if isinstance(docs, str) else len(docs) } {language} documents completed in {toc - tic:0.4f} seconds\")\n",
    "    return processed_docs\n",
    "\n",
    "\n",
    "\n",
    "def normalize_docs(docs, lang, with_stemming, minimum_words_per_doc):\n",
    "    if not docs:\n",
    "        return []\n",
    "    docs = [docs] if isinstance(docs, str) else docs\n",
    "    s = \" \"\n",
    "    normalised_docs = []\n",
    "    global nlp\n",
    "    if (nlp == None):\n",
    "        nlp = spacy.load(\"el_core_news_md\" if lang == 'greek' else \"en_core_web_md\")\n",
    "    to_be_processed_docs = remove_chars(docs)\n",
    "    if(lang == 'greek'):\n",
    "        to_be_processed_docs = keep_only_table_of_contents(to_be_processed_docs)\n",
    "        to_be_processed_docs = remove_punctuation_doc(to_be_processed_docs)\n",
    "    to_be_processed_docs, list_of_indexes_to_keep = replace_numbers_doc(to_be_processed_docs)\n",
    "    print(\"Removing Stopwords, Stemming: {} Lemma:{}, Removing short and common words...\".format(with_stemming, not with_stemming))\n",
    "    for index, doc in enumerate(to_be_processed_docs):\n",
    "        normalised_doc = None\n",
    "        doc = remove_unnecessary_spaces(doc)\n",
    "        bigrams_of_doc = nlp_bigrams(doc, nlp)\n",
    "        words = list(map(lambda word: word.lower().strip(), word_tokenize(doc)))\n",
    "        words.extend(bigrams_of_doc)\n",
    "        no_stop_words = remove_stopwords(words, lang)\n",
    "        if(with_stemming):\n",
    "            normalised_doc = stem(no_stop_words, index, lang, True)\n",
    "        else:\n",
    "            m_doc = nlp(s.join(no_stop_words))\n",
    "            normalised_doc = [token.lemma_ for token in m_doc]\n",
    "        normalised_doc = remove_short_words(normalised_doc, 5)\n",
    "        normalised_doc = remove_common_words(normalised_doc, lang)\n",
    "        normalised_docs.append(normalised_doc)\n",
    "    normalised_docs = list(filter(lambda doc: len(doc) >= minimum_words_per_doc, normalised_docs))\n",
    "    \n",
    "    return normalised_docs[0] if len(normalised_docs) == 1 else normalised_docs\n",
    "\n",
    "\n",
    "''' to be used only for the greek dataset '''\n",
    "def keep_only_table_of_contents(docs):\n",
    "    print(\"keep_only_table_of_contents...cut everything that is before the first time that periexomena if found\")\n",
    "    \"\"\"cut everything that is before the first time that periexomena if found\"\"\"\n",
    "    new_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        index = 0\n",
    "# #working only if periex is found once in the toc: index=re.search(\"περιεχ\", doc, re.IGNORECASE).start()\n",
    "        instances_array = [m.start() for m in re.finditer('περιεχ', doc, re.IGNORECASE)]\n",
    "        # print(i)\n",
    "        # print(instances_array)\n",
    "        if len(instances_array) == 1:\n",
    "            index = instances_array[0]\n",
    "        elif len(instances_array) == 2:\n",
    "            if instances_array[0] == 0:\n",
    "                index = instances_array[0]\n",
    "            else:\n",
    "                index = instances_array[1]\n",
    "        elif len(instances_array) == 0:\n",
    "            index = 0\n",
    "        else:\n",
    "            index = instances_array[0]\n",
    "        new_doc = doc[index:]\n",
    "        new_docs.append(new_doc)\n",
    "    return new_docs\n",
    "\n",
    "\n",
    "def remove_punctuation_doc(docs):\n",
    "    print(\"Remove punctuation from list of docs\")\n",
    "    \"\"\"Remove punctuation from list of docs\"\"\"\n",
    "    new_docs = []\n",
    "    for index, doc in enumerate(docs):\n",
    "        new_doc = re.sub(r'[^\\w\\s]', ' ', doc)\n",
    "        new_docs.append(new_doc)\n",
    "        # print(index)\n",
    "    return new_docs\n",
    "\n",
    "\n",
    "def trim(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_words.append(word.strip())\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def replace_numbers_doc(docs):\n",
    "    print(\"Replace all interger occurrences in list of docs\")\n",
    "    \"\"\"Replace all interger occurrences in list of docs\"\"\"\n",
    "    \"\"\"In order to know which docs are not empty after removing all number occurrences\"\"\"\n",
    "    list_of_indexes_to_keep = []\n",
    "    new_docs = []\n",
    "    regex = re.compile(r'\\b(?=[MDCLXVI]+\\b)M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})\\b')\n",
    "    for index, doc in enumerate(docs):\n",
    "        result = ''.join(i for i in doc if not i.isdigit())\n",
    "        result_no_romans = regex.sub(' ', result.upper())\n",
    "        if not result_no_romans == '':\n",
    "            list_of_indexes_to_keep.append(index)  # print(result)\n",
    "            new_docs.append(result_no_romans.lower())\n",
    "            \n",
    "    return new_docs, list_of_indexes_to_keep\n",
    "\n",
    "\n",
    "def remove_unnecessary_spaces(document):\n",
    "    return re.sub(' +', ' ', document)\n",
    "\n",
    "\n",
    "def nlp_bigrams(document, nlp):\n",
    "    if nlp is None:\n",
    "        nlp = spacy.load(\"el_core_news_md\")\n",
    "    nlp_doc = nlp(document)\n",
    "    return list(set(textacy.extract.utils.terms_to_strings(list(textacy.extract.basics.ngrams(nlp_doc, 2, min_freq=2)), by=\"orth\"))) \n",
    "\n",
    "\n",
    "def stop_words(lang):\n",
    "    stopwords_enhanced = stopwords.words(lang)\n",
    "    if lang == 'greek':\n",
    "        list_with_words_to_be_removed = ['της', 'ως', 'από', 'στα', 'τους', 'μια', 'ένας', 'ένα', 'μία', 'στον', 'στο', 'στη', 'στους', 'στις', 'στα']\n",
    "        stopwords_enhanced.extend(list_with_words_to_be_removed)\n",
    "    return stopwords_enhanced\n",
    "\n",
    "\n",
    "def remove_chars(docs):\n",
    "    print(\"remove chars not to be used and tokenize\")\n",
    "    \"\"\"remove chars not to be used and tokenize\"\"\"\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        s = re.sub('\\n', ' ', str(doc))  # for greek_aei dataset\n",
    "        s1 = re.sub('\\t', ' ', s)\n",
    "        s2 = re.sub('\\ufeff', ' ', s1)\n",
    "        s3 = re.sub('_', ' ', s2)\n",
    "        s4 = re.sub('β€', ' ', s3)  # for springer dataset\n",
    "        new_docs.append(s4)\n",
    "    return new_docs\n",
    "\n",
    "\n",
    "def remove_stopwords(words, lang):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        stopwords_enhanced = stop_words(lang) + stop_words('english')\n",
    "        if word not in stopwords_enhanced:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def stem(document, index, lang, log_to_file):\n",
    "    if lang == 'greek':\n",
    "        normalised_doc = replace_accentuation(document)\n",
    "    else:\n",
    "        normalised_doc = document\n",
    "    normalised_doc = stem_words(normalised_doc, lang, 'Snowball')\n",
    "    return normalised_doc\n",
    "\n",
    "\n",
    "def stem_words(words, lang, stemmer=None):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    if lang == 'english':\n",
    "        if stemmer == 'Lancaster':\n",
    "            stemmer = LancasterStemmer()\n",
    "        elif stemmer == 'Porter':\n",
    "            stemmer = PorterStemmer()\n",
    "        elif stemmer == 'Snowball':\n",
    "            stemmer = SnowballStemmer('english')\n",
    "    else:\n",
    "        stemmer = GreekStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "#         if (len(word.split())>1):\n",
    "#             print(\"bigram found-> {}.\".format(word))\n",
    "        trimmed = trim(word.split())\n",
    "        stemmed_sentece = [];\n",
    "        for trimmed_word in trimmed:\n",
    "            stemmed_sentece.append(stemmer.stem(trimmed_word.upper()))\n",
    "        stems.append(\"_\".join(stemmed_sentece).lower())    \n",
    "    return stems\n",
    "\n",
    "\n",
    "def replace_accentuation(words):\n",
    "    \"\"\"replace tonous\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "#         new_word = strip_accents(word)\n",
    "        new_word = remove_all_diacritics_with_diaer(word)\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_short_words(words, min_length):\n",
    "    \"\"\"Removes words less than the minimum threshold set\"\"\"\n",
    "    filtered_tokens = list(filter(lambda word: len(word) >= min_length, words));\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "def remove_common_words(words, lang):\n",
    "    if lang == 'greek':\n",
    "        list_with_common_words = ['ΠΕΡΙΕΧΟΜΕΝ', 'ΠΕΡΙΕΧ', 'ΑΣΚΗΣ', 'ΕΥΡΕΤΗΡ', 'ΕΙΣΑΓΩΓ', 'ΚΕΦΑΛΑΙ', 'ΒΙΒΛΙΟΓΡΑΦ', 'ΕΙΝΑΙ', 'ΠΡΟΛΟΓ', 'ΚΑΤΑΛΟΓ', 'ΕΚΔΟΤ', 'ΕΚΔΟΣ', 'ΜΕΤΑΦΡΑΣΤ',\n",
    "                                  'περιεχομεν', 'περιεχ', 'ασκησ', 'ευρετηρ', 'εισαγωγ', 'κεφαλαι', 'βιβλιογραφ', 'ειναι', 'ειν', 'προλογ', 'καταλογ', 'εκδοτ', 'εκδοσ', 'μεταφραστ', 'περιληψ',\n",
    "                                  'περιεχόμενο', 'άσκηση', 'ευρετήριο', 'εισαγωγη', 'κεφάλαιο', 'πρόλογος', 'κατάλογος', 'εκδότης', 'μεταφραστ΄ής',\n",
    "                                  'περιέχει', 'βιβλιογραφία', 'είναι', 'έκδοση', 'περίληψη', 'τελος', 'τροπο',\n",
    "                                  'περιεχομενο', 'ασκηση', 'ευρετηριο', 'εισαγωγη', 'κεφαλαιο', 'προλογος', 'καταλογος', 'εκδοτης', 'μεταφραστης',\n",
    "                                  'περιεχόμενα', 'ασκήσεις', 'ευρετήρια', 'εισαγωγές', 'κεφάλαια', 'πρόλογοι', 'κατάλογοι', 'εκδότες', 'μεταφραστές',\n",
    "                                  'περιεχομενα', 'ασκησεις', 'ευρετηρια', 'εισαγωγες', 'κεφαλαια', 'προλογοι', 'καταλογοι', 'εκδοτες', 'μεταφραστες',\n",
    "                                  'περιεχει', 'βιβλιογραφια', 'ειναι', 'εκδοση' , 'βιβλιο', 'βιβλιου', 'περιληψη', 'συνοψη',\n",
    "                                  'ελλην', 'εκδος', 'σημειωμ', 'συγγραφ', 'προσωπ', 'γιατι', 'ακομη', 'σχετικα', 'γενικα',\n",
    "                                  'μεταξυ', 'πρεπει', 'εχουν', 'υπαρχω', 'υπαρχεις', 'υπαρχει', 'υπαρχουν', 'κανεις',\n",
    "                                  'μπορω', 'μπορεις', 'μπορει', 'μπορουμε', 'μπορουν', 'γινεται', 'δηλαδη', 'εχουμε'\n",
    "                                  'αποτελω', 'αποτελεις', 'αποτελει', 'αποτελουν', 'πλεον', 'συχνα',\n",
    "                                  'αυτος', 'αυτη', 'αυτης', 'αυτο', 'αυτων', 'αυτου', 'αυτοι', 'αυτες', 'πολλα', 'πολλες΄',\n",
    "                                  'οποιος', 'οποια', 'οποιο', 'οποιοι', 'οποιες', 'ποιος', 'ποια', 'ποιο', 'ποιοι', 'ποιες', 'επισης', 'καθως', 'σημερα']\n",
    "    else:\n",
    "        list_with_common_words = ['introduction', 'conclusions', 'preface', 'index', 'references', 'foreward', 'acknowledgments', 'chapter', 'appendix', 'part', 'introduct', 'conclus', 'refer', 'discuss', 'result', 'suggest', 'appendic', ]\n",
    "    filtered_tokens = []\n",
    "    for w in words:\n",
    "        if (w not in list_with_common_words):\n",
    "            filtered_tokens.append(w)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90c00ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' function for the subjects (retrieval and processing)'''\n",
    "def get_preprocessed_subjects(subjects_file_name):\n",
    "    all_categories_df = get_subjects(file_name=subjects_file_name)\n",
    "    if (ALL_CATEGORIES):\n",
    "        categories_df = all_categories_df\n",
    "    else:\n",
    "        categories_df = all_categories_df[all_categories_df['category'].isin(['COMPUTER SCIENCE', 'TECH SCIENCES AND ENGINEERING', 'MATHEMATICS', 'HEALTH SCIENCES', 'CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])]    \n",
    "        physical_sciences = categories_df[categories_df['category'].isin(['CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])].content.values.tolist()\n",
    "        physical_sciences_content = \"\"\n",
    "        for idx in range(len(physical_sciences)): \n",
    "            physical_sciences_content += physical_sciences[idx][1:-1]\n",
    "        categories_df = categories_df.drop(categories_df[categories_df['category'].isin(['CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])].index)\n",
    "        categories_df = categories_df.append({'category':'PHYSICAL SCIENCES', 'content':'[' + physical_sciences_content + ']'}, ignore_index=True)\n",
    "    preprocessed_categories_df = preprocess_subjects(categories_df)\n",
    "    return preprocessed_categories_df\n",
    " \n",
    "\n",
    "''' Load Themes (Standard Documents) functions'''\n",
    "def init_subjects(docs_name, file_to_save, language):\n",
    "    wb = load_workbook(docs_name)\n",
    "    categories_dictionary = {}\n",
    "    for n, worksheet in enumerate(wb.worksheets):\n",
    "        dict_key = worksheet.title\n",
    "        cell_values = []\n",
    "        for col_cells in worksheet.iter_cols(min_col=1, max_col=1):\n",
    "            for cell in col_cells:\n",
    "                cell_values.append(cell.value)\n",
    "        normalized_categories = normalize_docs([SPACE.join(cell_values)], lang=language, with_stemming=STEMMING_ENABLED, minimum_words_per_doc=4)\n",
    "        categories_dictionary[dict_key] =  normalized_categories[0] if len(normalized_categories) == 1 else normalized_categories\n",
    "        print(categories_dictionary)\n",
    "        df = pd.DataFrame(categories_dictionary.items(), columns=['category', 'content'])\n",
    "        df.to_csv(file_to_save, sep=';',)\n",
    "\n",
    "        \n",
    "def get_subjects(file_name):\n",
    "    categories_xls_to_load = categories_xls if len(file_name) < 1 else os.path.join(os.path.abspath(''), file_name + '.xlsx')\n",
    "    categories_to_load = saved_categories_file if len(file_name) < 1 else os.path.join(os.path.abspath(''), file_name + '.txt')\n",
    "    if not os.path.isfile(categories_to_load):\n",
    "        init_subjects(categories_xls_to_load, categories_to_load, LANGUAGE)\n",
    "    df = load_subjects_csv(categories_to_load)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_subjects_csv(file_name):\n",
    "    return pd.read_csv(file_name, sep=';', index_col=0)\n",
    "\n",
    "\n",
    "def preprocess_subjects(categories_df):\n",
    "    for index, row in categories_df.iterrows():\n",
    "        row['content'] = preprocess(row['content'], LANGUAGE, STEMMING_ENABLED)\n",
    "    return categories_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb19a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Utility method for loading the springer dataset along\n",
    " containing the book titles and the tocs in pandas dataframe format.\n",
    " After loading, the data are split to training and test data.\n",
    " The analogy is 80% for the training data and 20% for the test data.\n",
    " \n",
    " Returns a dictionary of with key, value format: [bookId, toc]'''\n",
    "def load_split_data():\n",
    "    print(\"Loading Springer TOCs...\")\n",
    "    df = pd.read_pickle(\"springer_dataframe_26_categories.pkl\")\n",
    "    # df = pd.read_pickle(\"springer_dataframe_5_categories.pkl\")\n",
    "    df.to_csv(os.path.join(os.path.abspath(''), 'springer_all_toc.csv'), sep=';')\n",
    "    # df.to_csv(os.path.join(os.path.abspath(''), 'springer_dataframe_5_categories.csv'), sep=';')\n",
    "\n",
    "    train, test = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)\n",
    "    print(\"Splitting the pandas dataframe...\")\n",
    "    # For Default Springer Dataset\n",
    "    # return train[\"toc\"], test.index, test[\"toc\"]\n",
    "    # For 5 categories springer dataset\n",
    "    return dict(zip(train.title.tolist() + test.title.tolist(), train.toc.tolist() + test.toc.tolist()))\n",
    "\n",
    "''' Loads the indicative documents acting as test documents sample'''\n",
    "def load_indicative_docs():\n",
    "    xls = pd.ExcelFile(os.path.join(os.path.abspath(''), f'{name_by_categories_no(\"Indicative_Documents\", ALL_CATEGORIES)}.xlsx'))\n",
    "    return pd.read_excel(xls, sheet_name=None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9457d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' LDA Training Model Functions '''\n",
    "\n",
    "''' Method for creating a dictionary based on documents. '''\n",
    "def create_dictionary(documents, not_less_than, not_more_than):\n",
    "    print(\"Creating the dictionary and filtering extremes\")\n",
    "    dictionary = gensim.corpora.Dictionary(documents)\n",
    "    # Filter out tokens that appear in\n",
    "    # less than 15 documents (absolute number) or\n",
    "    # more than 0.5 documents (fraction of total corpus size, not the absolute number).\n",
    "    # After the above two steps, keep only the first 100000 most frequent tokens.\n",
    "    # keep_n=100000\n",
    "    dictionary.filter_extremes(no_below=not_less_than, no_above=not_more_than)\n",
    "    return dictionary\n",
    "\n",
    "''' Retrieves the optimum trained topic model'''\n",
    "def optimum_topic_modelling (train_documents, dictionary, limit, start, step):\n",
    "    print(\"Topic Modelling starting...\")\n",
    "    lda_model, best_number_of_topics = train_optimum_no_of_topics_LDA(dictionary, train_documents, limit, start, step)\n",
    "    print(\"Generated Topics: {}\".format(lda_model.print_topics()))\n",
    "    return lda_model, best_number_of_topics\n",
    "\n",
    "\n",
    "''' Method for executing the topic modelling based on a fixed number of topics.'''\n",
    "def default_topic_modelling (train_documents, dictionary, num_of_topics):\n",
    "    print(\"Topic Modelling starting...\")\n",
    "    lda_model = train_fixed_no_of_topics_LDA_model(dictionary=dictionary, train_documents=train_documents, num_of_topics=num_of_topics)\n",
    "    print(\"Generated Topics: {}\".format(lda_model.print_topics()))\n",
    "    return lda_model\n",
    "\n",
    "\n",
    "def train_fixed_no_of_topics_LDA_model(dictionary, train_documents, num_of_topics):\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in train_documents]\n",
    "    return gensim.models.wrappers.LdaMallet(mallet_path, corpus=bow_corpus, num_topics=num_of_topics, id2word=dictionary)\n",
    "\n",
    "\n",
    "# Method for training the LDA Model    \n",
    "def train_optimum_no_of_topics_LDA(dictionary, train_documents, limit, start, step):\n",
    "    print(\"Training the LDA Model\")\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in train_documents]\n",
    "    lda_model, best_number_of_topics = find_optimal_model(dictionary, bow_corpus, train_documents, limit, start, step)\n",
    "#     ''' Visualize the optimum number of topics'''\n",
    "#     visualize_topics(lda_model=lda_model, corpus=bow_corpus, dictionary=dictionary)\n",
    "    return lda_model, best_number_of_topics\n",
    "\n",
    "''' Finds the optimal trained model based on coherence'''\n",
    "def find_optimal_model(dictionary, corpus, texts, limit, start, step):\n",
    "    print(\"Finding the optimal LDA model...\")\n",
    "    topics_range, models_list, coherence_values = compute_coherence_values(dictionary, corpus, texts, limit, start, step)\n",
    "    return get_optimal_model(topics_range, models_list, coherence_values)\n",
    "\n",
    "\n",
    "def get_optimal_model(num_of_topics, model_list, coherence_values):\n",
    "    coherence_values_array = np.array(coherence_values)\n",
    "    model_array = np.array(model_list)\n",
    "    num_of_topics_array = np.array(num_of_topics)\n",
    "    peaks_idxs, _ = find_peaks(coherence_values_array, prominence=peaks_prominence)\n",
    "    coherence_peaks = np.array([coherence_values[peak_idx] for peak_idx in peaks_idxs])\n",
    "    middle_of = lambda x: x[math.trunc(len(x) / 2)]  if(len(x) % 2 == 1) else x[math.trunc(len(x) / 2) - 1] \n",
    "    optimal_peak = middle_of(coherence_peaks)\n",
    "#     max_peak = np.amax(coherence_peaks)\n",
    "    optimal_peak_idx = np.where(coherence_values_array == optimal_peak)\n",
    "    print(\"optimal number of topics: {} with optimal coherence: {}\\n\".format(num_of_topics_array[optimal_peak_idx], coherence_values_array[optimal_peak_idx]))\n",
    "    optimal_model_tuple = model_array[optimal_peak_idx]\n",
    "    return optimal_model_tuple[0], num_of_topics_array[optimal_peak_idx][0]\n",
    "\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    print(\"Calculating models and coherences...\")\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    \n",
    "    topics_range = range(start, limit, step) \n",
    "    return topics_range, model_list, coherence_values\n",
    "\n",
    "\n",
    "def visualize_topics(lda_model, corpus, dictionary):\n",
    "    model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(lda_model)\n",
    "    visualisation = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
    "    pyLDAvis.save_html(visualisation, 'LDA_Visualization.html')\n",
    "    pyLDAvis.display(visualisation)\n",
    "\n",
    "\n",
    "\n",
    "def visualize_topics_coherence(coherence_values, topics_range):\n",
    "    coherence_values_array = np.array(coherence_values)\n",
    "    peaks, _ = find_peaks(coherence_values_array, prominence=peaks_prominence)\n",
    "    stepped_peaks = peaks * topics_range.step\n",
    "    offseted_peaks = stepped_peaks + topics_range.start\n",
    "    plt.plot(offseted_peaks, coherence_values_array[peaks], \"xk\")\n",
    "    plt.plot(topics_range, coherence_values_array)\n",
    "    plt.xlabel(\"Num Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c82c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Retrieval of information from the trained LDA Model'''\n",
    "\n",
    "#Retrieves in a dictionary the topic id and its words distribution in the format:\n",
    "#[topicId, topic_word_distribution]\n",
    "def get_words_per_topic(lda_model, dictionary):\n",
    "    topics_with_words = dict(lda_model.show_topics(num_topics=-1, num_words=len(dictionary), formatted=False))\n",
    "    for topic_id, all_words in topics_with_words.items():\n",
    "        filtered = [word_distribution_tuple for (index, word_distribution_tuple) in enumerate(all_words) if word_distribution_tuple[1] > 0]\n",
    "        topics_with_words[topic_id] = filtered\n",
    "    return topics_with_words\n",
    "\n",
    "\n",
    "def get_number_of_topics(lda_model):\n",
    "    return len(lda_model.print_topics(num_topics=-1))\n",
    "\n",
    "\n",
    "def topic_distribution_of_document (lda_model, dictionary, document):\n",
    "    bow_vector = dictionary.doc2bow(document)\n",
    "    return sorted(lda_model[bow_vector], key=lambda tup:-1 * tup[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d254d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Formulas for transforming the LDA Model to Themes Distribution'''\n",
    "\n",
    "\n",
    "def p_themes_given_document(preprocessed_doc, lda_model, dictionary, preprocessed_categories_df, p_themes_given_topics_df):\n",
    "    theme_labels = preprocessed_categories_df.category.values.tolist()\n",
    "    themes_content = flatten(preprocessed_categories_df.content.values.tolist())\n",
    "    themes_docs_dict = dict((key, 0) for key in theme_labels)\n",
    "    document_topic_distribution = topic_distribution_of_document(lda_model, dictionary, preprocessed_doc)\n",
    "    for theme_label in theme_labels:\n",
    "        theme, theme_content = get_theme(theme_label, preprocessed_categories_df)\n",
    "        for topic_id, probability_topic_given_document in document_topic_distribution:\n",
    "            themes_docs_dict[theme_label] += probability_topic_given_document * p_themes_given_topics_df.loc[topic_id, theme_label]\n",
    "    return themes_docs_dict\n",
    "\n",
    "''' Function for  calculating a datatable with the distribution of themes over topics.\n",
    "    the datatable structure is nxm where n the number of topics and m the number of themes.\n",
    "'''\n",
    "\n",
    "\n",
    "def p_themes_given_topics(preprocessed_categories_df, topics_with_words):\n",
    "    topic_ids = topics_with_words.keys()\n",
    "    theme_labels = preprocessed_categories_df.category.values.tolist()\n",
    "    themes_content = flatten(preprocessed_categories_df.content.values.tolist())\n",
    "    themes_given_topics_df = pd.DataFrame(index=topic_ids, columns=theme_labels)\n",
    "    for theme_label in theme_labels:\n",
    "        theme, theme_content = get_theme(theme_label, preprocessed_categories_df)\n",
    "        for topic_id in topic_ids:\n",
    "            themes_given_topics_df.at[topic_id, theme_label] = p_theme_given_topic(theme_content, themes_content, topic_id, topics_with_words)\n",
    "    return themes_given_topics_df\n",
    "\n",
    "def p_topics_given_themes(lda_model, dictionary, preprocessed_categories_df):\n",
    "    theme_labels = preprocessed_categories_df.category.values.tolist()\n",
    "    topic_ids = range(get_number_of_topics(lda_model))\n",
    "    themes_dist = themes_distribution(lda_model, dictionary, preprocessed_categories_df)\n",
    "    p_topics_give_themes_df = pd.DataFrame(index=topic_ids, columns=theme_labels)\n",
    "    for topic_id in topic_ids:\n",
    "        for theme_label in theme_labels:\n",
    "            p_topics_give_themes_df.at[topic_id, theme_label] = p_topic_given_document(topic_id, themes_dist[theme_label])\n",
    "    return p_topics_give_themes_df\n",
    "\n",
    "def p_theme_given_topic(document, corpus, topic_id, topics_with_words):\n",
    "    document_unique_words = set(document)\n",
    "    probability_document_given_topic = 0\n",
    "    for document_unique_word in document_unique_words:\n",
    "        probability_document_given_topic += p_word_given_topic(document_unique_word, topic_id, topics_with_words) * p_document_given_word(document_unique_word, document, corpus)\n",
    "    return probability_document_given_topic\n",
    "\n",
    "def p_documents_matrix(preprocessed_corpus_dict, preprocessed_categories_df):\n",
    "    p_documents_matrix_df = pd.DataFrame(index=preprocessed_corpus_dict.keys(), columns=['p_document'])\n",
    "    words_from_D = flatten(preprocessed_categories_df.content.values.tolist())\n",
    "    for document_title, preprocessed_doc in preprocessed_corpus_dict.items():\n",
    "        p_documents_matrix_df.at[document_title, 'p_document'] = accumulated_frequency_of(preprocessed_doc, words_from_D)\n",
    "    return normalize_columns_df(p_documents_matrix_df)\n",
    "\n",
    "'''Pr{tk}=ΣmPr{tk|bm}Pr{bm}'''\n",
    "\n",
    "def p_topics_from_formulas(lda_model, dictionary, preprocessed_corpus_dict, p_documents_df):\n",
    "    topic_ids = range(get_number_of_topics(lda_model))\n",
    "    p_topics_dict = dict (zip(topic_ids, [0 for i in topic_ids]))\n",
    "    for document_title, preprocessed_doc in preprocessed_corpus_dict.items():\n",
    "        document_topic_distribution = topic_distribution_of_document(lda_model, dictionary, preprocessed_doc)\n",
    "        for topic_id in topic_ids:\n",
    "            p_topics_dict[topic_id] += p_topic_given_document(topic_id, document_topic_distribution) * p_documents_df.loc[document_title, 'p_document']\n",
    "    return p_topics_dict\n",
    "\n",
    "def p_topics_from_corpus_to_lda(lda_model, dictionary, preprocessed_corpus_bag_of_words):\n",
    "    topic_ids = range(get_number_of_topics(lda_model))\n",
    "    corpus_distribution = topic_distribution_of_document(lda_model, dictionary, preprocessed_corpus_bag_of_words);\n",
    "    p_topics_dict = {}\n",
    "    for topic_id in topic_ids:\n",
    "        p_topics_dict[topic_id] = p_topic_given_document(topic_id, corpus_distribution)\n",
    "    return p_topics_dict\n",
    "\n",
    "def p_topic_given_document (topic_id, document_topic_distribution):\n",
    "    topic_given_document_tuples = [topic_probability_tuple for (index, topic_probability_tuple) in enumerate(document_topic_distribution) if topic_probability_tuple[0] == topic_id]\n",
    "    if (not topic_given_document_tuples):\n",
    "        return None\n",
    "    topic_given_document_tuple = topic_given_document_tuples[0]\n",
    "    probability_topic_given_document = topic_given_document_tuple[1]\n",
    "    return probability_topic_given_document\n",
    "\n",
    "def p_theme_given_document(preprocessed_categories_df, document):\n",
    "    document_unique_words = set(document)\n",
    "    theme_labels = preprocessed_categories_df.category.values.tolist()\n",
    "    S = flatten(preprocessed_categories_df.content.values.tolist())\n",
    "    probability_of_theme_given_doc_df = pd.DataFrame(index=theme_labels, columns=['p_theme_given_doc'])\n",
    "    for theme_label in theme_labels:\n",
    "        theme, theme_content = get_theme(categories_df=preprocessed_categories_df,theme_label=theme_label)\n",
    "        probability_theme_given_document = 0\n",
    "        for document_unique_word in document_unique_words:\n",
    "            p_doc_given_word = p_document_given_word(document_unique_word, theme_content, S)\n",
    "            p_word_given_doc = p_word_given_document(document_unique_word, document)\n",
    "            probability_theme_given_document += p_doc_given_word * p_word_given_doc\n",
    "        probability_of_theme_given_doc_df.at[theme_label, 'p_theme_given_doc'] = probability_theme_given_document \n",
    "    return normalize_columns_rounded_df(probability_of_theme_given_doc_df)\n",
    "\n",
    "def p_document_given_word(word, document, corpus):\n",
    "    word_joint_document = frequency_of(word, document)\n",
    "    word_in_corpus = frequency_of(word, corpus)\n",
    "    if word_in_corpus == 0:\n",
    "        return 0\n",
    "    return word_joint_document / word_in_corpus\n",
    "\n",
    "def p_word_given_document(word, corpus):\n",
    "    word_joint_document = frequency_of(word, corpus)\n",
    "    words_of_corpus = len(corpus)\n",
    "    return word_joint_document / words_of_corpus\n",
    "\n",
    "\n",
    "''' Function for retrieving the probability of a word given a topic\n",
    "    word: the word to evaluate\n",
    "    \n",
    "    topic_id: the topic the word would appear\n",
    "    \n",
    "    topics_with_words: the dictionary of topic ids as keys \n",
    "    and a list of tuples (word: word_probability) \n",
    "    for the corresponding topic id \n",
    "    @see: get_words_per_topic'''\n",
    "\n",
    "\n",
    "def p_word_given_topic (word, topic_id, topics_with_words):\n",
    "    words_of_topic = topics_with_words[topic_id]\n",
    "    word_given_topic_tuples = [word_probability_tuple for (index, word_probability_tuple) in enumerate(words_of_topic) if word_probability_tuple[0] == word]\n",
    "    if not word_given_topic_tuples or len(word_given_topic_tuples) > 1:\n",
    "        return 0\n",
    "    word_given_topic_tuple = word_given_topic_tuples[0]\n",
    "    probability_word_given_topic = word_given_topic_tuple[1]\n",
    "    return probability_word_given_topic\n",
    "\n",
    "\n",
    "def evaluate_p_theme(p_topics_given_themes_df, p_themes_given_topic_df, p_topics_dict):\n",
    "    topic_ids = p_topics_dict.keys()\n",
    "    theme_labels = p_themes_given_topic_df.columns.values.tolist()\n",
    "    p_themes = pd.DataFrame(index=topic_ids, columns=theme_labels)\n",
    "    for topic_id in topic_ids:\n",
    "        for theme_label in theme_labels:\n",
    "            p_themes.at[topic_id,theme_label] = p_topics_dict[topic_id] * (p_themes_given_topic_df.loc[topic_id, theme_label] / p_topics_given_themes_df.loc[topic_id, theme_label])\n",
    "    return normalize_rounded_df(p_themes)\n",
    "\n",
    "def p_themes_by_formulas(preprocessed_categories_df, preprocessed_corpus_dict):\n",
    "    theme_labels = preprocessed_categories_df.category.values.tolist()\n",
    "    all_themes = flatten(preprocessed_categories_df.content.values.tolist())\n",
    "    corpus = flatten(preprocessed_corpus_dict.values())\n",
    "    probability_of_themes_df = pd.DataFrame(index=theme_labels, columns=['p_theme'])\n",
    "    for theme_label in theme_labels:\n",
    "        theme, theme_content = get_theme(theme_label, preprocessed_categories_df)\n",
    "        probability_of_themes_df.at[theme_label, 'p_theme'] = p_theme(theme_content, all_themes, corpus)\n",
    "    return normalize_columns_rounded_df(probability_of_themes_df)\n",
    "\n",
    "def p_theme(theme, themes_corpus, corpus):\n",
    "    document_unique_words = set(theme)\n",
    "    probability_of_document = 0\n",
    "    for document_unique_word in document_unique_words:\n",
    "        probability_of_document += p_document_given_word(document_unique_word, theme, themes_corpus) * p_word_given_document(document_unique_word, corpus)\n",
    "    return probability_of_document\n",
    "\n",
    "\n",
    "def get_theme(theme_label, categories_df):\n",
    "    for index, row in categories_df.iterrows():\n",
    "        if(row['category'] == theme_label):\n",
    "            return row['category'], row['content']\n",
    "    return None\n",
    "\n",
    "def themes_distribution(lda_model, dictionary, preprocessed_categories_df):\n",
    "    themes_topic_distribution_dict = {}\n",
    "    theme_labels = preprocessed_categories_df.category.values.tolist()\n",
    "    for theme_label in theme_labels:\n",
    "        theme, theme_content = get_theme(theme_label, preprocessed_categories_df)\n",
    "        themes_topic_distribution_dict[theme_label] = topic_distribution_of_document(lda_model, dictionary, theme_content)\n",
    "    return themes_topic_distribution_dict\n",
    "\n",
    "\n",
    "def document_coverage(document, corpus):\n",
    "    document_unique_words = set(document)\n",
    "    words_found = 0 \n",
    "    for word in document_unique_words:\n",
    "        if word in corpus:\n",
    "            words_found+=1\n",
    "    return words_found / len(document_unique_words)\n",
    "\n",
    "def words_count_of_document(document, corpus):\n",
    "    words_found = 0\n",
    "    for word in document:\n",
    "        if(word in corpus):\n",
    "            words_found+=1\n",
    "    return words_found\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81801e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Helper methods for the execution of the LDA'''\n",
    "def export_subjects_to_json(preprocessed_categories_df):\n",
    "    categories_dict = dict(zip(preprocessed_categories_df.category.values.tolist(), preprocessed_categories_df.content.values.tolist()))\n",
    "    with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(\"preprocessed_categories\",ALL_CATEGORIES)}.json', 'w', encoding='utf8') as fp:\n",
    "        json.dump(categories_dict, fp, ensure_ascii=False)\n",
    "        \n",
    "\n",
    "\n",
    "def export_themes_given_documents(dictionary, lda_model, preprocessed_categories_df, p_themes_given_topics_df, docs_dict, file_name):\n",
    "    theme_labels = preprocessed_categories_df.category.values.tolist()\n",
    "    themes_for_documents_df = pd.DataFrame(index=docs_dict.keys(), columns=theme_labels)\n",
    "    for document_name, book_toc in docs_dict.items():\n",
    "        themes_given_document_dict = p_themes_given_document(preprocessed_doc=book_toc, lda_model=lda_model, dictionary=dictionary, preprocessed_categories_df=preprocessed_categories_df, p_themes_given_topics_df=p_themes_given_topics_df)\n",
    "        for theme_label, themes_distribution_for_document in themes_given_document_dict.items():\n",
    "            themes_for_documents_df.at[document_name, theme_label] = themes_distribution_for_document\n",
    "    \n",
    "    themes_for_documents_df = normalize_rounded_df(themes_for_documents_df)\n",
    "    themes_for_documents_df.to_csv(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(file_name,ALL_CATEGORIES)}.csv'), sep=';')\n",
    "    replace_file_char_with_char(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(file_name,ALL_CATEGORIES)}.csv'), '.', ',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Execution Of The LDA Theme Distribution Solution part 1.\n",
    "In this part the themes distribution of all the documents and the test documents will be exported'''\n",
    "\n",
    "all_categories_df = get_subjects(file_name='Categories_engish')\n",
    "categories_df = pd.DataFrame()\n",
    "''' Merge to Physical Sciences '''\n",
    "if (ALL_CATEGORIES):\n",
    "    categories_df = all_categories_df\n",
    "else:\n",
    "    categories_df = all_categories_df[all_categories_df['category'].isin(['COMPUTER SCIENCE', 'TECH SCIENCES AND ENGINEERING', 'MATHEMATICS', 'HEALTH SCIENCES', 'CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])]    \n",
    "    physical_sciences = categories_df[categories_df['category'].isin(['CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])].content.values.tolist()\n",
    "    physical_sciences_content = \"\"\n",
    "    for idx in range(len(physical_sciences)): \n",
    "        physical_sciences_content += physical_sciences[idx][1:-1]\n",
    "    categories_df = categories_df.drop(categories_df[categories_df['category'].isin(['CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])].index)\n",
    "    categories_df = categories_df.append({'category':'PHYSICAL SCIENCES', 'content':'[' + physical_sciences_content + ']'}, ignore_index=True)\n",
    "preprocessed_categories_df = preprocess_subjects(categories_df)\n",
    "export_subjects_to_json(preprocessed_categories_df)\n",
    "\n",
    "''' Load the dataset '''\n",
    "docs_dict = load_split_data()\n",
    "\n",
    "''' Setting the dictionary and training of the LDA Model.\n",
    "There are two options based on the variable OPTIMIMUM_NUMBER_OF_TOPICS_ENABLED (default value = True):\n",
    "1) Find the optimal number of topics\n",
    "2) Set a fixed number of topics\n",
    "\n",
    "After the dictionary is set and the lda model is trained they are cached for future use.\n",
    "'''\n",
    "print (\"Springer Dictionary Path: \"+springer_dictionary_path)\n",
    "if os.path.isfile(springer_dictionary_path):\n",
    "    print(f'{springer_dictionary_path} found, loading it to memory' )\n",
    "    dictionary = gensim.corpora.dictionary.Dictionary.load(springer_dictionary_path)\n",
    "else:\n",
    "    processed_dict_docs = preprocess(docs=all_categories_df.content.tolist(), language=LANGUAGE, with_stemming=STEMMING_ENABLED)\n",
    "    dictionary = create_dictionary(processed_dict_docs, not_less_than, not_more_than)\n",
    "    dictionary.save(springer_dictionary_path)\n",
    "    print('Springer dictionary created')\n",
    "\n",
    "print (\"Springer LDA Model Path: \"+springer_model_path)\n",
    "if os.path.isfile(springer_model_path):\n",
    "    print(f'{springer_model_path} found, loading it to memory' )\n",
    "    lda_model = LdaMallet.load(springer_model_path)\n",
    "else:\n",
    "    preprocessed_corpus_dict = load_csv_to_dict(name_by_categories_no(\"preprocessed_corpus\",ALL_CATEGORIES))\n",
    "    # preprocessed_train_docs = preprocess(docs=preprocessed_corpus_dict.values(), language=LANGUAGE, with_stemming=STEMMING_ENABLED)\n",
    "    print('Train documents preprocessed')\n",
    "    if(OPTIMIMUM_NUMBER_OF_TOPICS_ENABLED):\n",
    "        print(f'Topic modelling starting with the discovery of optimal number of topics based on coherence.')\n",
    "        lda_model, best_num_of_topics = optimum_topic_modelling(preprocessed_corpus_dict.values(), dictionary, limit, start, step)\n",
    "        num_of_closest_topics = best_num_of_topics - 1\n",
    "    else:\n",
    "        print(f'Topic modelling starting with a fixed number of topics: {num_of_closest_topics}')\n",
    "        lda_model = default_topic_modelling(train_documents=preprocessed_train_docs, dictionary=dictionary, num_of_topics=fixed_number_of_topics)\n",
    "    print('LDA Model Trained')\n",
    "    lda_model.save(springer_model_path)\n",
    "indicative_docs_dict = load_indicative_docs()\n",
    "all_indicative_docs_df = pd.DataFrame(columns=['BookID', 'Primary Category', 'Other Categories'])\n",
    "for sheet_title, indicative_docs_df in indicative_docs_dict.items():\n",
    "    all_indicative_docs_df = all_indicative_docs_df.append(indicative_docs_df)\n",
    "\n",
    "\n",
    "print(f'Number of topics {get_number_of_topics(lda_model)}')    \n",
    "theme_labels = preprocessed_categories_df.category.values.tolist()\n",
    "topics_with_words = get_words_per_topic(lda_model=lda_model, dictionary=dictionary)\n",
    "save_to_file(f'{os.getcwd()}/results/{name_by_categories_no(\"topics_with_words\",ALL_CATEGORIES)}', topics_with_words)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b94c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Preprocess or load cached preprocessed corpus.'''\n",
    "preprocessed_corpus_dict = {}\n",
    "\n",
    "if os.path.isfile(f'{os.getcwd()}/{name_by_categories_no(\"preprocessed_corpus\",ALL_CATEGORIES)}.csv'):\n",
    "    preprocessed_corpus_dict = load_csv_to_dict(name_by_categories_no(\"preprocessed_corpus\",ALL_CATEGORIES))\n",
    "else: \n",
    "    for document_title in docs_dict.keys():\n",
    "        preprocessed_document_content = preprocess(docs_dict[document_title], LANGUAGE, STEMMING_ENABLED)\n",
    "        if len(preprocessed_document_content) > 0:\n",
    "            preprocessed_corpus_dict[document_title] = preprocess(docs_dict[document_title], LANGUAGE, STEMMING_ENABLED)\n",
    "    save_to_file(name_by_categories_no(\"preprocessed_corpus\", ALL_CATEGORIES), preprocessed_corpus_dict)\n",
    "    print(f'preprocessed_corpus saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03455e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Calculate p_themes, p_topics'''\n",
    "p_topics_dict_from_corpus = p_topics_from_corpus_to_lda(lda_model, dictionary, flatten(preprocessed_corpus_dict.values()))\n",
    "save_to_file(name_by_categories_no(\"p_topics_from_corpus_to_lda\",ALL_CATEGORIES), p_topics_dict_from_corpus)\n",
    "print(\"p_topics_from_corpus_to_lda created\")\n",
    "\n",
    "p_themes_given_topic_df = p_themes_given_topics(preprocessed_categories_df=preprocessed_categories_df, topics_with_words=topics_with_words)\n",
    "p_themes_given_topic_df = normalize_rounded_df(p_themes_given_topic_df)\n",
    "p_themes_given_topic_df.to_csv(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(\"p_themes_given_topics\",ALL_CATEGORIES)}.csv'), sep=';')\n",
    "replace_file_char_with_char(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(\"p_themes_given_topics\",ALL_CATEGORIES)}.csv'), '.', ',')\n",
    "print('p_themes_given_topic created')\n",
    "\n",
    "p_topics_given_themes_df = p_topics_given_themes(lda_model, dictionary, preprocessed_categories_df)\n",
    "save_to_file(name_by_categories_no(\"p_topics_given_themes\",ALL_CATEGORIES), p_topics_given_themes_df)\n",
    "print('p_topics_given_themes created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5ee919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_frequency_in_doc_sorted(document):\n",
    "    word_frequency = {}\n",
    "    for word in set(document):\n",
    "        word_frequency[word] = document.count(word)\n",
    "    return dict(sorted(word_frequency.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "\n",
    "def Coverage_of_B_by_S(all_categories):\n",
    "    all_categories_df = get_subjects(file_name='default_Categories_engish')\n",
    "    if (all_categories):\n",
    "        categories_df = all_categories_df\n",
    "    else:\n",
    "        categories_df = all_categories_df[all_categories_df['category'].isin(['COMPUTER SCIENCE', 'TECH SCIENCES AND ENGINEERING', 'MATHEMATICS', 'HEALTH SCIENCES', 'CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])]    \n",
    "        physical_sciences = categories_df[categories_df['category'].isin(['CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])].content.values.tolist()\n",
    "        physical_sciences_content = \"\"\n",
    "        for idx in range(len(physical_sciences)): \n",
    "            physical_sciences_content += physical_sciences[idx][1:-1]\n",
    "        categories_df = categories_df.drop(categories_df[categories_df['category'].isin(['CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])].index)\n",
    "        categories_df = categories_df.append({'category':'PHYSICAL SCIENCES', 'content':'[' + physical_sciences_content + ']'}, ignore_index=True)\n",
    "    preprocessed_categories_df = preprocess_subjects(categories_df)\n",
    "    theme_labels = preprocessed_categories_df.category.values.tolist()\n",
    "    preprocessed_corpus_dict = load_csv_to_dict(name_by_categories_no(\"preprocessed_corpus\", ALL_CATEGORIES))\n",
    "    preprocessed_corpus = flatten(preprocessed_corpus_dict.values())\n",
    "    size_of_preprocessed_corpus = len(preprocessed_corpus)\n",
    "    S = flatten(preprocessed_categories_df.content.values.tolist())\n",
    "    coverage_df = pd.DataFrame(index=preprocessed_categories_df.category.values.tolist(), columns=[\"Coverage_of_B_by_Si\"])\n",
    "    for theme_label in theme_labels:\n",
    "        theme, theme_content = get_theme(theme_label, preprocessed_categories_df)\n",
    "        words_frequency_of_theme = words_frequency_in_doc_sorted(theme_content)\n",
    "        coverage_of_theme = 0\n",
    "        for word, frequency_in_doc in words_frequency_of_theme.items():\n",
    "            coverage_of_theme += (frequency_in_doc / frequency_of(word, S)) * (frequency_of(word, preprocessed_corpus) / size_of_preprocessed_corpus)\n",
    "        coverage_df.loc[theme, 'Coverage_of_B_by_Si'] = coverage_of_theme\n",
    "    coverage_df.to_csv(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/Coverage_of_B_by_S.csv'), sep=';') \n",
    "\n",
    "def Coverage_of_S_by_B(all_categories):\n",
    "    all_categories_df = get_subjects(file_name='default_Categories_engish')\n",
    "    if (all_categories):\n",
    "        categories_df = all_categories_df\n",
    "    else:\n",
    "        categories_df = all_categories_df[all_categories_df['category'].isin(['COMPUTER SCIENCE', 'TECH SCIENCES AND ENGINEERING', 'MATHEMATICS', 'HEALTH SCIENCES', 'CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])]    \n",
    "        physical_sciences = categories_df[categories_df['category'].isin(['CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])].content.values.tolist()\n",
    "        physical_sciences_content = \"\"\n",
    "        for idx in range(len(physical_sciences)): \n",
    "            physical_sciences_content += physical_sciences[idx][1:-1]\n",
    "        categories_df = categories_df.drop(categories_df[categories_df['category'].isin(['CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])].index)\n",
    "        categories_df = categories_df.append({'category':'PHYSICAL SCIENCES', 'content':'[' + physical_sciences_content + ']'}, ignore_index=True)\n",
    "    preprocessed_categories_df = preprocess_subjects(categories_df)\n",
    "    theme_labels = preprocessed_categories_df.category.values.tolist()\n",
    "    preprocessed_corpus_dict = load_csv_to_dict(name_by_categories_no(\"preprocessed_corpus\", ALL_CATEGORIES))\n",
    "    preprocessed_corpus = flatten(preprocessed_corpus_dict.values())\n",
    "    S = flatten(preprocessed_categories_df.content.values.tolist())\n",
    "    size_of_S = len(S)\n",
    "    coverage_df = pd.DataFrame(index=preprocessed_categories_df.category.values.tolist(), columns=[\"Coverage_of_Si_by_B\"])\n",
    "    coverage_df['Coverage_of_Si_by_B'] = 0\n",
    "    for title, document in preprocessed_corpus_dict.items():\n",
    "        words_frequency_of_document = words_frequency_in_doc_sorted(document)\n",
    "        for word, frequency_in_doc in words_frequency_of_document.items():\n",
    "            for theme_label in theme_labels:\n",
    "                theme, theme_content = get_theme(theme_label, preprocessed_categories_df)\n",
    "                coverage_df.loc[theme, 'Coverage_of_Si_by_B']+= (frequency_in_doc / frequency_of(word, preprocessed_corpus)) * (frequency_of(word, theme_content) / len(theme_content))\n",
    "    coverage_df.to_csv(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/Coverage_of_S_by_B.csv'), sep=';') \n",
    "\n",
    "def coverage_of_Document_by_S(all_categories, preprocessed_doc):\n",
    "    all_categories_df = get_subjects(file_name='default_Categories_engish')\n",
    "    if (all_categories):\n",
    "        categories_df = all_categories_df\n",
    "    else:\n",
    "        categories_df = all_categories_df[all_categories_df['category'].isin(['COMPUTER SCIENCE', 'TECH SCIENCES AND ENGINEERING', 'MATHEMATICS', 'HEALTH SCIENCES', 'CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])]    \n",
    "        physical_sciences = categories_df[categories_df['category'].isin(['CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])].content.values.tolist()\n",
    "        physical_sciences_content = \"\"\n",
    "        for idx in range(len(physical_sciences)): \n",
    "            physical_sciences_content += physical_sciences[idx][1:-1]\n",
    "        categories_df = categories_df.drop(categories_df[categories_df['category'].isin(['CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])].index)\n",
    "        categories_df = categories_df.append({'category':'PHYSICAL SCIENCES', 'content':'[' + physical_sciences_content + ']'}, ignore_index=True)\n",
    "    preprocessed_categories_df = preprocess_subjects(categories_df)\n",
    "    theme_labels = preprocessed_categories_df.category.values.tolist()\n",
    "    size_of_preprocessed_doc = len(preprocessed_doc)\n",
    "    S = flatten(preprocessed_categories_df.content.values.tolist())\n",
    "    coverage_df = pd.DataFrame(index=preprocessed_categories_df.category.values.tolist(), columns=[\"Coverage_of_B_by_Si\"])\n",
    "    for theme_label in theme_labels:\n",
    "        theme, theme_content = get_theme(theme_label, preprocessed_categories_df)\n",
    "        words_frequency_of_theme = words_frequency_in_doc_sorted(theme_content)\n",
    "        coverage_of_theme = 0\n",
    "        for word, frequency_in_doc in words_frequency_of_theme.items():\n",
    "            coverage_of_theme += (frequency_in_doc / frequency_of(word, S)) * (frequency_of(word, preprocessed_doc) / size_of_preprocessed_doc)\n",
    "        coverage_df.loc[theme, 'Coverage_of_Document_by_Si'] = coverage_of_theme\n",
    "    coverage_df.to_csv(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(\"Coverage_of_Document_by_Si\", all_categories)}.csv'), sep=';') \n",
    "\n",
    "   \n",
    "def coverage_of_S_by_Document(all_categories, preprocessed_corpus_dict, document_name, preprocessed_doc):\n",
    "    all_categories_df = get_subjects(file_name='default_Categories_engish')\n",
    "    if (all_categories):\n",
    "        categories_df = all_categories_df\n",
    "    else:\n",
    "        categories_df = all_categories_df[all_categories_df['category'].isin(['COMPUTER SCIENCE', 'TECH SCIENCES AND ENGINEERING', 'MATHEMATICS', 'HEALTH SCIENCES', 'CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])]    \n",
    "        physical_sciences = categories_df[categories_df['category'].isin(['CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])].content.values.tolist()\n",
    "        physical_sciences_content = \"\"\n",
    "        for idx in range(len(physical_sciences)): \n",
    "            physical_sciences_content += physical_sciences[idx][1:-1]\n",
    "        categories_df = categories_df.drop(categories_df[categories_df['category'].isin(['CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])].index)\n",
    "        categories_df = categories_df.append({'category':'PHYSICAL SCIENCES', 'content':'[' + physical_sciences_content + ']'}, ignore_index=True)\n",
    "    preprocessed_categories_df = preprocess_subjects(categories_df)\n",
    "    theme_labels = preprocessed_categories_df.category.values.tolist()\n",
    "    preprocessed_corpus = flatten(preprocessed_corpus_dict.values())\n",
    "    coverage_df = pd.DataFrame(index=preprocessed_categories_df.category.values.tolist(), columns=[\"Coverage_of_Si_by_B\"])\n",
    "    coverage_df['Coverage_of_Si_by_Document'] = 0\n",
    "    words_frequency_of_document = words_frequency_in_doc_sorted(preprocessed_doc)\n",
    "    for word, frequency_in_doc in words_frequency_of_document.items():\n",
    "        for theme_label in theme_labels:\n",
    "            theme, theme_content = get_theme(theme_label, preprocessed_categories_df)\n",
    "            coverage_df.loc[theme, 'Coverage_of_Si_by_Document']+= (frequency_in_doc / frequency_of(word, preprocessed_doc)) * (frequency_of(word, theme_content) / len(theme_content))\n",
    "    coverage_df.to_csv(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(\"Coverage_of_Si_by_\"+document_name, all_categories)}.csv'), sep=';') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' Themes distribution for all the documents'''\n",
    "with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(\"preprocessed_all_docs\", ALL_CATEGORIES)}.json', 'w', encoding='utf8') as fp:\n",
    "    json.dump(preprocessed_corpus_dict, fp, ensure_ascii=False)\n",
    "    \n",
    "export_themes_given_documents(dictionary, lda_model, preprocessed_categories_df, p_themes_given_topic_df , preprocessed_corpus_dict, name_by_categories_no(\"all_docs_distributions\", ALL_CATEGORIES))\n",
    "\n",
    "print('Springer subject distributions of all documents extracted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Themes distribution for the test documents'''\n",
    "test_docs_dict = {}\n",
    "\n",
    "for index, indicative_doc_row in all_indicative_docs_df.iterrows():\n",
    "    document_name = indicative_doc_row[\"BookID\"]\n",
    "    test_docs_dict[document_name] = preprocessed_corpus_dict[document_name]\n",
    "with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(\"preprocessed_test_docs\", ALL_CATEGORIES)}.json', 'w', encoding='utf8') as fp:\n",
    "    json.dump(test_docs_dict, fp, ensure_ascii=False)\n",
    "\n",
    "export_themes_given_documents(dictionary, lda_model, preprocessed_categories_df, p_themes_given_topic_df, test_docs_dict, name_by_categories_no(\"test_docs_distributions\", ALL_CATEGORIES))\n",
    "\n",
    "\n",
    "print('Springer subject distributions of test documents extracted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad1d5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Execution of the LDA solution part 2\n",
    "In this section the exported results will be processed in order to\n",
    "be exported to files that can be viewed and evaluated visually by the dashboards\n",
    "in the results/visual folder.'''\n",
    "\n",
    "''' What follows is helper methods for data export'''\n",
    "\n",
    "'''Replaces the , with . in order for all the docs (title and content) to be in a standarized format.\n",
    "This process is for the visualization of the documents and does not affect the exported themes distribution\n",
    "in csv format.'''\n",
    "def standarize_document_names(file_name):\n",
    "    with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(file_name, ALL_CATEGORIES)}.json', encoding='utf8') as json_file:\n",
    "        docs_dict = json.load(json_file)\n",
    "        docs_dict = dict((key.replace(',', '.'), value) for (key, value) in docs_dict.items())\n",
    "    with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(file_name, ALL_CATEGORIES)}.json', 'w', encoding='utf8') as fp:\n",
    "        json.dump(docs_dict, fp, ensure_ascii=False)\n",
    "\n",
    "def calculate_dist_distances(row_to_check, candidate_row, column_names, docs_dict):\n",
    "    to_check_distribution = convert_list_to_float(replace_in_list(row_to_check[column_names].values.tolist(), \",\", \".\"))\n",
    "    candidate_distribution = convert_list_to_float(replace_in_list(candidate_row[column_names].values.tolist(), \",\", \".\"))\n",
    "    distribution_distance = distance.jensenshannon(to_check_distribution, candidate_distribution)\n",
    "    return abs(1000 if distribution_distance == 0 else distribution_distance)\n",
    "\n",
    "def convert_list_to_float(list_to_covert):\n",
    "    return  list(map(float, list_to_covert))\n",
    "\n",
    "def replace_in_list(list_to_replace, replace_from, replace_to):\n",
    "    return list(map(lambda x: str.replace(x, replace_from, replace_to), list_to_replace))\n",
    "\n",
    "def sort_dict(dict_to_sort, descending):\n",
    "    return dict(sorted(dict_to_sort.items(), key=lambda item: item[1], reverse=descending))\n",
    "\n",
    "def mix_couples_of_documents(groups_of_lists_of_documents_to_mix):\n",
    "    groups_combinations = list(itertools.combinations(groups_of_lists_of_documents_to_mix, 2))\n",
    "    mixed_documents = []\n",
    "    for group_combination in groups_combinations:\n",
    "        items_to_retrieve = [random.choice(group_combination[0]), random.choice(group_combination[1])]\n",
    "        mixed_documents.append(items_to_retrieve)\n",
    "    return mixed_documents\n",
    "        \n",
    "''' Export the p_theme_given topic to json for visual representation'''\n",
    "def export_p_theme_given_topic_to_json(p_theme_given_topic_df):\n",
    "    p_theme_give_topic_dict = {}\n",
    "    column_names = p_theme_given_topic_df.columns.values.tolist()\n",
    "    for topic_id, row in p_theme_given_topic_df.iterrows():\n",
    "        p_theme_give_topic_dict[topic_id] = dict(zip(column_names, row[column_names]))\n",
    "    with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(\"p_themes_given_topics.json\",ALL_CATEGORIES)}', 'w', encoding='utf8') as fp:\n",
    "        json.dump(p_theme_give_topic_dict, fp, ensure_ascii=False)\n",
    "\n",
    "''' Export the p_theme_given documents to json for visual representation'''        \n",
    "def export_p_theme_given_documents_to_json(themes_distribution_over_docs_df, filename):\n",
    "    p_themes_given_doc_dict = {}\n",
    "    column_names = themes_distribution_over_docs_df.columns.values.tolist()\n",
    "    for document_name, row in themes_distribution_over_docs_df.iterrows():\n",
    "        p_themes_given_doc_dict[document_name] = dict(zip(column_names, row[column_names]))\n",
    "    with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(filename,ALL_CATEGORIES)}.json', 'w', encoding='utf8') as fp:\n",
    "        json.dump(p_themes_given_doc_dict, fp, ensure_ascii=False)\n",
    "        \n",
    "''' export the 3 closest documents to json format for visuala representation'''\n",
    "def export_closest_documents(themes_distribution_over_docs_df, themes_distribution_over_all_docs_df, num_of_closest_documents, preprocessed_all_docs_json_path, closest_documents_file_name):\n",
    "    column_names = themes_distribution_over_docs_df.columns.values.tolist()\n",
    "    closest_documents_df = pd.DataFrame(index=themes_distribution_over_docs_df.index, columns=\n",
    "                                        [f'theme_{k}_of_document_to_check' for k in range(3)] + \n",
    "                                        [f'closest_document_name_{j}' for j in range(num_of_closest_documents)] + \n",
    "                                        [f'closest_document_distance_{i}' for i in range(num_of_closest_documents)] + \n",
    "                                        [f'theme_{k}_to_closest_document_0' for k in range(3)] + \n",
    "                                        [f'closest_document_0_exact_match', f'closest_document_0_match'] + \n",
    "                                        [f'theme_{k}_to_closest_document_1' for k in range(3)] + \n",
    "                                        [f'closest_document_1_exact_match', f'closest_document_1_match'] + \n",
    "                                        [f'theme_{k}_to_closest_document_2' for k in range(3)] + \n",
    "                                        [f'closest_document_2_exact_match', f'closest_document_2_match'])\n",
    "    with open(f'{preprocessed_all_docs_json_path}', encoding='utf8') as json_file:\n",
    "        docs_dict = json.load(json_file)\n",
    "    for document_to_check_name, document_to_check_distribution_row in themes_distribution_over_docs_df.iterrows():\n",
    "        distances_from_document_to_check_df = pd.DataFrame(index=themes_distribution_over_all_docs_df.index, columns=['Distance'])\n",
    "        distances_from_document_to_check_df['Distance'] = themes_distribution_over_all_docs_df.apply(lambda row: calculate_dist_distances(document_to_check_distribution_row, row, column_names, docs_dict), axis=1)\n",
    "        distances_from_document_to_check_df = distances_from_document_to_check_df. sort_values(by=[f'Distance'])\n",
    "        document_distance_index = 0\n",
    "        for document_name, document_distance in distances_from_document_to_check_df.head(num_of_closest_documents).iterrows():\n",
    "            closest_documents_df.at[document_to_check_name, f'closest_document_name_{document_distance_index}'] = document_name\n",
    "            closest_documents_df.at[document_to_check_name, f'closest_document_distance_{document_distance_index}'] = document_distance['Distance']\n",
    "            document_distance_index += 1\n",
    "    closest_documents_dict = {}\n",
    "    closest_documents_names_columns = [f'closest_document_name_{j}' for j in range(num_of_closest_documents)]\n",
    "    closest_documents_distances_columns = [f'closest_document_distance_{i}' for i in range(num_of_closest_documents)]\n",
    "    themes_columns_of_document_to_check = [f'theme_{k}_of_document_to_check' for k in range(3)]\n",
    "    \n",
    "    for document_name, closest_documents in closest_documents_df.iterrows():\n",
    "        document_to_check_dist = themes_distribution_over_docs_df.loc[document_name]\n",
    "        document_to_check_distribution_dict = dict(zip(document_to_check_dist.index.tolist(), document_to_check_dist.values.tolist()))\n",
    "        sorted_document_to_check_distribution_dict = sort_dict(document_to_check_distribution_dict, True)                                          \n",
    "        closest_documents[themes_columns_of_document_to_check] = list(sorted_document_to_check_distribution_dict.keys())[:3]\n",
    "        \n",
    "        closest_document_names = closest_documents[closest_documents_names_columns].values.tolist()\n",
    "        i = 0\n",
    "        for closest_document_name in closest_document_names:\n",
    "            closest_document_distribution = themes_distribution_over_all_docs_df.loc[closest_document_name]\n",
    "            closest_document_distribution_dict = dict(zip(closest_document_distribution.index.tolist(), closest_document_distribution.values.tolist()))\n",
    "            sorted_distribution_dict = sort_dict(closest_document_distribution_dict, True)                                          \n",
    "            closest_documents[[f'theme_0_to_closest_document_{i}', f'theme_1_to_closest_document_{i}', f'theme_2_to_closest_document_{i}']] = list(sorted_distribution_dict.keys())[:3]\n",
    "            \n",
    "            documents_to_check_themes_list = closest_documents[themes_columns_of_document_to_check].values.tolist()\n",
    "            closest_document_themes_list = closest_documents[[f'theme_0_to_closest_document_{i}', f'theme_1_to_closest_document_{i}', f'theme_2_to_closest_document_{i}']].values.tolist()\n",
    "            exact_match = documents_to_check_themes_list == closest_document_themes_list\n",
    "            match = set(documents_to_check_themes_list) == set(closest_document_themes_list)\n",
    "            closest_documents_df.at[document_name, f'closest_document_{i}_exact_match'] = exact_match            \n",
    "            closest_documents_df.at[document_name, f'closest_document_{i}_match'] = match\n",
    "            i += 1\n",
    "    closest_documents_df.to_csv(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/visual/{name_by_categories_no(closest_documents_file_name, ALL_CATEGORIES)}.csv'), sep=';')\n",
    "    \n",
    "    for document_name, closest_documents in closest_documents_df.iterrows():\n",
    "        closest_document_names = closest_documents[closest_documents_names_columns].values.tolist()\n",
    "        closest_document_distances = closest_documents[closest_documents_distances_columns].values.tolist()\n",
    "        closest_documents_dict[document_name] = dict(zip(closest_document_names, closest_document_distances))\n",
    "    with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(closest_documents_file_name, ALL_CATEGORIES)}.json', 'w', encoding='utf8') as fp:\n",
    "        json.dump(closest_documents_dict, fp, ensure_ascii=False)\n",
    "        \n",
    "\n",
    "''' export the 2 closest documents to json format for visuala representation'''\n",
    "def export_closest_2_documents(themes_distribution_over_docs_df, themes_distribution_over_all_docs_df, num_of_closest_documents=2):\n",
    "    column_names = themes_distribution_over_docs_df.columns.values.tolist()\n",
    "    closest_documents_df = pd.DataFrame(index=themes_distribution_over_docs_df.index, columns=\n",
    "                                        [f'theme_{k}_of_document_to_check' for k in range(3)] + \n",
    "                                        [f'closest_document_name_{j}' for j in range(num_of_closest_documents)] + \n",
    "                                        [f'closest_document_distance_{i}' for i in range(num_of_closest_documents)] + \n",
    "                                        [f'theme_{k}_to_closest_document_0' for k in range(3)] + \n",
    "                                        [f'closest_document_0_exact_match', f'closest_document_0_match'] + \n",
    "                                        [f'theme_{k}_to_closest_document_1' for k in range(3)] + \n",
    "                                        [f'closest_document_1_exact_match', f'closest_document_1_match'])\n",
    "    with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(\"preprocessed_all_docs\",ALL_CATEGORIES)}.json', encoding='utf8') as json_file:\n",
    "        docs_dict = json.load(json_file)\n",
    "    for document_to_check_name, document_to_check_distribution_row in themes_distribution_over_docs_df.iterrows():\n",
    "        distances_from_document_to_check_df = pd.DataFrame(index=themes_distribution_over_all_docs_df.index, columns=['Distance'])\n",
    "        distances_from_document_to_check_df['Distance'] = themes_distribution_over_all_docs_df.apply(lambda row: calculate_dist_distances(document_to_check_distribution_row, row, column_names, docs_dict), axis=1)\n",
    "        distances_from_document_to_check_df = distances_from_document_to_check_df. sort_values(by=[f'Distance'])\n",
    "        document_distance_index = 0\n",
    "        for document_name, document_distance in distances_from_document_to_check_df.head(num_of_closest_documents).iterrows():\n",
    "            closest_documents_df.at[document_to_check_name, f'closest_document_name_{document_distance_index}'] = document_name\n",
    "            closest_documents_df.at[document_to_check_name, f'closest_document_distance_{document_distance_index}'] = document_distance['Distance']\n",
    "            document_distance_index += 1\n",
    "    closest_documents_names_columns = [f'closest_document_name_{j}' for j in range(num_of_closest_documents)]\n",
    "    themes_columns_of_document_to_check = [f'theme_{k}_of_document_to_check' for k in range(2)]\n",
    "    \n",
    "    for document_name, closest_documents in closest_documents_df.iterrows():\n",
    "        document_to_check_dist = themes_distribution_over_docs_df.loc[document_name]\n",
    "        document_to_check_distribution_dict = dict(zip(document_to_check_dist.index.tolist(), document_to_check_dist.values.tolist()))\n",
    "        sorted_document_to_check_distribution_dict = sort_dict(document_to_check_distribution_dict, True)                                          \n",
    "        closest_documents[themes_columns_of_document_to_check] = list(sorted_document_to_check_distribution_dict.keys())[:2]\n",
    "        \n",
    "        closest_document_names = closest_documents[closest_documents_names_columns].values.tolist()\n",
    "        i = 0\n",
    "        for closest_document_name in closest_document_names:\n",
    "            closest_document_distribution = themes_distribution_over_all_docs_df.loc[closest_document_name]\n",
    "            closest_document_distribution_dict = dict(zip(closest_document_distribution.index.tolist(), closest_document_distribution.values.tolist()))\n",
    "            sorted_distribution_dict = sort_dict(closest_document_distribution_dict, True)                                          \n",
    "            closest_documents[[f'theme_0_to_closest_document_{i}', f'theme_1_to_closest_document_{i}']] = list(sorted_distribution_dict.keys())[:2]\n",
    "            \n",
    "            documents_to_check_themes_list = closest_documents[themes_columns_of_document_to_check].values.tolist()\n",
    "            closest_document_themes_list = closest_documents[[f'theme_0_to_closest_document_{i}', f'theme_1_to_closest_document_{i}']].values.tolist()\n",
    "            exact_match = documents_to_check_themes_list == closest_document_themes_list\n",
    "            match = set(documents_to_check_themes_list) == set(closest_document_themes_list)\n",
    "            closest_documents_df.at[document_name, f'closest_document_{i}_exact_match'] = exact_match            \n",
    "            closest_documents_df.at[document_name, f'closest_document_{i}_match'] = match\n",
    "            i += 1\n",
    "    closest_documents_df.to_csv(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/visual/{name_by_categories_no(\"closest_2_documents_with_themes\",ALL_CATEGORIES)}.csv'), sep=';')\n",
    "\n",
    "def export_mixed_documents(lda_model, dictionary, topics_with_words, p_themes_given_topics_df):\n",
    "    theme_labels = p_themes_given_topics_df.columns.values.tolist()\n",
    "    numeric_p_theme_given_topic_df = p_themes_given_topics_df.replace(',', '.', regex=True)\n",
    "    numeric_p_theme_given_topic_df = numeric_p_theme_given_topic_df[theme_labels].astype('float')\n",
    "    all_categories_df = get_subjects(file_name='Categories_engish')\n",
    "    if (ALL_CATEGORIES):\n",
    "        categories_df = all_categories_df\n",
    "    else:\n",
    "        categories_df = all_categories_df[all_categories_df['category'].isin(['COMPUTER SCIENCE', 'TECH SCIENCES AND ENGINEERING', 'MATHEMATICS', 'HEALTH SCIENCES', 'CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])]    \n",
    "        physical_sciences = categories_df[categories_df['category'].isin(['CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])].content.values.tolist()\n",
    "        physical_sciences_content = \"\"\n",
    "        for idx in range(len(physical_sciences)): \n",
    "            physical_sciences_content += physical_sciences[idx][1:-1]\n",
    "        categories_df = categories_df.drop(categories_df[categories_df['category'].isin(['CHEMISTRY', 'PHYSICS', 'ASTRONOMY AND ASTROPHYSICS'])].index)\n",
    "        categories_df = categories_df.append({'category':'PHYSICAL SCIENCES', 'content':'[' + physical_sciences_content + ']'}, ignore_index=True)\n",
    "    preprocessed_categories_df = preprocess_subjects(categories_df)\n",
    " \n",
    "    with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(\"preprocessed_test_docs\",ALL_CATEGORIES)}.json', encoding='utf8') as json_file:\n",
    "        docs_dict = json.load(json_file)\n",
    "    documents_to_evaluate = {}\n",
    "    if(ALL_CATEGORIES == False):\n",
    "        MATHEMATICS_DOCS = replace_in_list(['Differential and Difference Equations with Applications', 'Functional Analysis and Evolution Equations'], ',', '.')\n",
    "        HEALTH_SCIENCE_DOCS = replace_in_list(['Oncology', 'Atlas of Genetic Diagnosis and Counseling', 'Oncology'], ',', '.')\n",
    "        \n",
    "        documents_to_evaluate[\"MATHEMATICS\"] = MATHEMATICS_DOCS\n",
    "        documents_to_evaluate[\"HEALTH_SCIENCE\"] = HEALTH_SCIENCE_DOCS\n",
    "  \n",
    "    else:\n",
    "        MATHEMATICS_DOCS = replace_in_list(['Fourier Series, Fourier Transform and Their Applications to Mathematical Physics', 'Convergence and Summability of Fourier Transforms and Hardy Spaces', 'Advances in Nonlinear Analysis via the Concept of Measure of Noncompactness'], ',', '.')\n",
    "        COMPUTER_SCIENCE_DOCS = replace_in_list(['Distributed Computing ', 'Knowledge-Based and Intelligent Information and Engineering Systems', 'Life System Modeling and Intelligent Computing'], ',', '.')\n",
    "        HEALTH_SCIENCE_DOCS = replace_in_list(['Pathology of the Gastrointestinal Tract', 'Pulmonary Hypertension in Adult Congenital Heart Disease', 'Pulmonary Vasculature Redox Signaling in Health and Disease'], ',', '.')\n",
    "        ENGINEERING_DOCS = replace_in_list(['Handbook of Manufacturing Engineering and Technology', 'Quality of Service in Heterogeneous Networks', 'Experimental and Applied Mechanics Volume 6'], ',', '.')\n",
    "        FINANCE_DOCS = replace_in_list(['Corporate Governance in Developing Economies', 'Principles of Public Finance', 'Inflation Dynamics in South Africa'], ',', '.')\n",
    "        documents_to_evaluate[\"MATHEMATICS\"] = MATHEMATICS_DOCS\n",
    "        documents_to_evaluate[\"COMPUTER_SCIENCE\"] = COMPUTER_SCIENCE_DOCS\n",
    "        documents_to_evaluate[\"HEALTH_SCIENCE\"] = HEALTH_SCIENCE_DOCS\n",
    "        documents_to_evaluate[\"ENGINEERING\"] = ENGINEERING_DOCS\n",
    "        documents_to_evaluate[\"FINANCE\"] = FINANCE_DOCS\n",
    "    mixed_documents = mix_couples_of_documents(documents_to_evaluate.values())\n",
    "    mixed_documents_names = [\"_\".join([str(mixed_document[0]), str(mixed_document[1])]) for mixed_document in mixed_documents]\n",
    "    mixed_documents_df = pd.DataFrame(index=mixed_documents_names, columns=theme_labels)\n",
    "    mixed_documents_df.index.astype(str, copy=False)\n",
    "    for mixed_document_couple in mixed_documents:\n",
    "        preprocessed_mixed_doc = docs_dict[mixed_document_couple[0]] + docs_dict[mixed_document_couple[1]]\n",
    "        mixed_document_idx = \"_\".join([str(mixed_document_couple[0]), str(mixed_document_couple[1])])\n",
    "        themes_given_document_dict = p_themes_given_document(preprocessed_doc=preprocessed_mixed_doc, lda_model=lda_model, dictionary=dictionary, preprocessed_categories_df=preprocessed_categories_df, p_themes_given_topics_df=numeric_p_theme_given_topic_df)\n",
    "        for theme_label, themes_distribution_for_mixed_document in themes_given_document_dict.items(): \n",
    "            mixed_documents_df.at[mixed_document_idx, theme_label] = themes_distribution_for_mixed_document \n",
    "    mixed_documents_df = normalize_rounded_df(mixed_documents_df)\n",
    "    mixed_documents_df.to_csv(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(\"mixed_docs\",ALL_CATEGORIES)}.csv'), sep=';')\n",
    "    replace_file_char_with_char(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(\"mixed_docs\",ALL_CATEGORIES)}.csv'), '.', ',')\n",
    "    \n",
    "    mixed_docs_dict = { \"_\".join([str(mix_doc[0]), str(mix_doc[1])]):docs_dict[mix_doc[0]] + docs_dict[mix_doc[1]] for mix_doc in mixed_documents}\n",
    "    with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(\"preprocessed_mixed_docs\",ALL_CATEGORIES)}.json', 'w', encoding='utf8') as fp:\n",
    "        json.dump(mixed_docs_dict, fp, ensure_ascii=False)\n",
    "    \n",
    "    p_themes_given_mixed_doc_dict = {}\n",
    "    column_names = mixed_documents_df.columns.values.tolist()\n",
    "    \n",
    "    for document_name, row in mixed_documents_df.iterrows():\n",
    "        p_themes_given_mixed_doc_dict[document_name] = dict(zip(column_names, row[column_names]))\n",
    "    with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(\"p_themes_given_mixed_documents\",ALL_CATEGORIES)}.json', 'w', encoding='utf8') as fp:\n",
    "        json.dump(p_themes_given_mixed_doc_dict, fp, ensure_ascii=False)\n",
    "\n",
    "        \n",
    "def export_mixed_document_with_closest_documents(lda_model, dictionary, topics_with_words, p_theme_given_topic_df, themes_distribution_over_docs_df, themes_distribution_over_all_docs_df):\n",
    "    theme_labels = p_theme_given_topic_df.columns.values.tolist()\n",
    "    numeric_p_theme_given_topic_df = p_theme_given_topic_df.replace(',', '.', regex=True)\n",
    "    numeric_p_theme_given_topic_df = numeric_p_theme_given_topic_df[theme_labels].astype('float')\n",
    "    num_of_closest_documents = 3\n",
    "    closest_documents_df = pd.DataFrame(index=themes_distribution_over_docs_df.index, columns=\n",
    "                                        [f'closest_document_name_{j}' for j in range(num_of_closest_documents)] + \n",
    "                                        [f'closest_document_distance_{i}' for i in range(num_of_closest_documents)])\n",
    "    with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(\"preprocessed_all_docs\",ALL_CATEGORIES)}.json', encoding='utf8') as json_file:\n",
    "        docs_dict = json.load(json_file)\n",
    "    math_document_name = 'Fourier Series, Fourier Transform and Their Applications to Mathematical Physics'.replace(',', '.')\n",
    "    health_science_document_name = 'Pathology of the Gastrointestinal Tract'\n",
    "    \n",
    "    for document_to_check_name in [math_document_name, health_science_document_name]:\n",
    "        distances_from_document_to_check_df = pd.DataFrame(index=themes_distribution_over_all_docs_df.index, columns=['Distance'])\n",
    "        distances_from_document_to_check_df['Distance'] = themes_distribution_over_all_docs_df.apply(lambda row: calculate_dist_distances(themes_distribution_over_docs_df.loc[document_to_check_name], row, theme_labels, docs_dict), axis=1)\n",
    "        distances_from_document_to_check_df = distances_from_document_to_check_df. sort_values(by=[f'Distance'])\n",
    "        document_distance_index = 0\n",
    "        for document_name, document_distance in distances_from_document_to_check_df.head(num_of_closest_documents).iterrows():\n",
    "            closest_documents_df.at[document_to_check_name, f'closest_document_name_{document_distance_index}'] = document_name.replace(',', '.')\n",
    "            closest_documents_df.at[document_to_check_name, f'closest_document_distance_{document_distance_index}'] = document_distance['Distance']\n",
    "            document_distance_index += 1\n",
    "        \n",
    "    math_closest_documents = closest_documents_df.loc[math_document_name, [f'closest_document_name_{j}' for j in range(num_of_closest_documents)]].values.tolist()\n",
    "    health_science_closest_documents = closest_documents_df.loc[health_science_document_name, [f'closest_document_name_{j}' for j in range(num_of_closest_documents)]].values.tolist()\n",
    "    mixed_documents_names = [math_document_name + '_' + health_science_document_name]\n",
    "    for i in range(num_of_closest_documents):\n",
    "        closest_math_name = math_closest_documents[i]\n",
    "        closest_health_science_name = health_science_closest_documents[i]\n",
    "        mixed_document_name = closest_math_name + '_' + closest_health_science_name\n",
    "        mixed_documents_names.append(mixed_document_name)\n",
    "    \n",
    "    mixed_documents_df = pd.DataFrame(index=[f'{i}_{mixed_documents_names[i]}' for i in range(len(mixed_documents_names))], columns=theme_labels)\n",
    "         \n",
    "    ''' Closest documents to mixed document'''\n",
    "    for i in range(len(mixed_documents_names)): \n",
    "        mixed_document_name = mixed_documents_names[i]\n",
    "        preprocessed_mixed_doc = docs_dict[mixed_document_name.split('_')[0]] + docs_dict[mixed_document_name.split('_')[1]]\n",
    "        themes_given_document_dict = p_themes_given_document(preprocessed_doc=preprocessed_mixed_doc, lda_model=lda_model, dictionary=dictionary, preprocessed_categories_df=preprocessed_categories_df, p_themes_given_topics_df=numeric_p_theme_given_topic_df)\n",
    "        for theme_label, themes_distribution_for_mixed_document in themes_given_document_dict.items(): \n",
    "            mixed_documents_df.at[f'{i}_{mixed_document_name}', theme_label] = themes_distribution_for_mixed_document\n",
    "    \n",
    "    mixed_documents_df = normalize_rounded_df(mixed_documents_df)\n",
    "    mixed_documents_df.to_csv(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(\"mixed_closest_docs\",ALL_CATEGORIES)}.csv'), sep=';')\n",
    "    replace_file_char_with_char(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(\"mixed_closest_docs\",ALL_CATEGORIES)}.csv'), '.', ',')\n",
    "           \n",
    "    mixed_docs_dict = { f'{mixed_documents_names.index(mix_doc_name)}_{mix_doc_name}':docs_dict[mix_doc_name.split('_')[0]] + docs_dict[mix_doc_name.split('_')[1]] for mix_doc_name in mixed_documents_names}\n",
    "    with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(\"preprocessed_mixed_closest_docs\",ALL_CATEGORIES)}.json', 'w', encoding='utf8') as fp:\n",
    "        json.dump(mixed_docs_dict, fp, ensure_ascii=False)\n",
    "    \n",
    "    p_themes_given_mixed_doc_dict = {}\n",
    "    column_names = mixed_documents_df.columns.values.tolist()\n",
    "    \n",
    "    for document_name, row in mixed_documents_df.iterrows():\n",
    "        p_themes_given_mixed_doc_dict[document_name] = dict(zip(column_names, row[column_names]))\n",
    "    with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(\"p_themes_given_mixed_closest_documents\",ALL_CATEGORIES)}.json', 'w', encoding='utf8') as fp:\n",
    "        json.dump(p_themes_given_mixed_doc_dict, fp, ensure_ascii=False)\n",
    "\n",
    "def sample(documents_dict, documents_percentages, number_of_words):\n",
    "    document_label_samples = []\n",
    "    for i in range(number_of_words):\n",
    "        document_label_idx = np.random.choice(np.arange(0, len(documents_dict.values())), p=documents_percentages)\n",
    "        document_label = list(documents_dict)[document_label_idx]\n",
    "        document_label_samples.append(document_label)\n",
    "    \n",
    "    with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(\"artificial_doc_terms_frequency\",ALL_CATEGORIES)}.json', 'w', encoding='utf8') as fp:\n",
    "        json.dump(words_frequency_in_doc_sorted(document_label_samples), fp, ensure_ascii=False)\n",
    "        \n",
    "    sampled_document = []\n",
    "    for document_label in document_label_samples:\n",
    "        random_word = random.choice(documents_dict[document_label])\n",
    "        sampled_document.append(random_word)\n",
    "    return sampled_document\n",
    "\n",
    "def sample_artificial_document():\n",
    "    dictionary = gensim.corpora.dictionary.Dictionary.load(springer_dictionary_path)\n",
    "    lda_model = LdaMallet.load(springer_model_path)\n",
    "    topics_with_words = topics_with_words = get_words_per_topic(lda_model=lda_model, dictionary=dictionary)\n",
    "    preprocessed_categories_df = get_preprocessed_subjects(subjects_file_name='default_Categories_engish')\n",
    "    p_themes_given_topic_df = p_themes_given_topics(preprocessed_categories_df=preprocessed_categories_df, topics_with_words=topics_with_words)\n",
    "    math_label, MATH = get_theme(\"MATHEMATICS\", preprocessed_categories_df)\n",
    "    economic_label, ECONOMIC = get_theme(\"ECONOMIC SCIENCES\", preprocessed_categories_df) if ALL_CATEGORIES else get_theme(\"PHYSICAL SCIENCES\", preprocessed_categories_df)\n",
    "    health_label, HEALTH = get_theme(\"HEALTH SCIENCES\", preprocessed_categories_df)\n",
    "    \n",
    "    artificial_doc = sample(dict(zip([math_label,economic_label, health_label],[MATH,ECONOMIC,HEALTH])), [0.5,0.3,0.2], 3000)\n",
    "    print(artificial_doc)\n",
    "    artificial_doc_distribution = p_themes_given_document(preprocessed_doc=artificial_doc, lda_model=lda_model, dictionary=dictionary, preprocessed_categories_df=preprocessed_categories_df, p_themes_given_topics_df=p_themes_given_topic_df)\n",
    "    themes_for_documents_df = pd.DataFrame(index=['artificial_doc'], columns=artificial_doc_distribution.keys())\n",
    "    for theme_label, themes_distribution_for_document in artificial_doc_distribution.items():\n",
    "        themes_for_documents_df.at['artificial_doc', theme_label] = themes_distribution_for_document\n",
    "    \n",
    "    themes_for_documents_df = normalize_rounded_df(themes_for_documents_df)\n",
    "    themes_for_documents_df.to_csv(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(\"artificial_doc_distribution\",ALL_CATEGORIES)}.csv'), sep=';')\n",
    "    replace_file_char_with_char(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(\"artificial_doc_distribution\",ALL_CATEGORIES)}.csv'), '.', ',')\n",
    "    column_names = themes_for_documents_df.columns.values.tolist()\n",
    "    artificial_doc_themes_distribution_dict = {}\n",
    "    for doc_name, row in themes_for_documents_df.iterrows():\n",
    "        artificial_doc_themes_distribution_dict[doc_name] = dict(zip(column_names, row[column_names]))\n",
    "    \n",
    "    with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(\"artificial_doc_distribution\",ALL_CATEGORIES)}.json', 'w', encoding='utf8') as fp:\n",
    "        json.dump(artificial_doc_themes_distribution_dict, fp, ensure_ascii=False)\n",
    "    \n",
    "    preprocessed_artificial_doc_dict = {}\n",
    "    preprocessed_artificial_doc_dict['artificial_doc'] = artificial_doc\n",
    "    with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(\"preprocessed_artificial_doc\",ALL_CATEGORIES)}.json', 'w', encoding='utf8') as fp:\n",
    "        json.dump(preprocessed_artificial_doc_dict, fp, ensure_ascii=False)\n",
    "    S = flatten(preprocessed_categories_df.content.values.tolist())\n",
    "    coverage_of_document_by_S = words_count_of_document(artificial_doc, S) / len(artificial_doc)\n",
    "    print(f'\\n Coverage of Artificial Document: {coverage_of_document_by_S}')\n",
    "    # coverage_df = themes_for_documents_df.apply(lambda row: coverage_of_document_by_S * row, axis=1)\n",
    "    # print(coverage_df)\n",
    "    # coverage_df.to_csv(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(\"artificial_doc_coverage\",ALL_CATEGORIES)}.csv'), sep=';')\n",
    "    # replace_file_char_with_char(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(\"artificial_doc_coverage\",ALL_CATEGORIES)}.csv'), '.', ',')\n",
    "\n",
    "def export_standard_category_distribution(subject_name):\n",
    "    dictionary = gensim.corpora.dictionary.Dictionary.load(springer_dictionary_path)\n",
    "    lda_model = LdaMallet.load(springer_model_path)\n",
    "    topics_with_words = topics_with_words = get_words_per_topic(lda_model=lda_model, dictionary=dictionary)\n",
    "    preprocessed_categories_df = get_preprocessed_subjects(subjects_file_name='default_Categories_engish')\n",
    "    theme_labels = preprocessed_categories_df.category.values.tolist()\n",
    "    p_themes_given_topic_df = p_themes_given_topics(preprocessed_categories_df=preprocessed_categories_df, topics_with_words=topics_with_words)\n",
    "    subject_label, subject_content = get_theme(subject_name, preprocessed_categories_df)\n",
    "\n",
    "    s_i_distribution = p_themes_given_document(preprocessed_doc=subject_content, lda_model=lda_model, dictionary=dictionary, preprocessed_categories_df=preprocessed_categories_df, p_themes_given_topics_df=p_themes_given_topic_df)\n",
    "    print(s_i_distribution)\n",
    "    themes_for_documents_df = pd.DataFrame(index=[subject_name], columns=theme_labels)\n",
    "    for theme_label, themes_distribution_for_document in s_i_distribution.items():\n",
    "        themes_for_documents_df.at[subject_name, theme_label] = themes_distribution_for_document\n",
    "    themes_for_documents_df = normalize_rounded_df(themes_for_documents_df)\n",
    "    themes_for_documents_df.to_csv(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(subject_name+\"_distribution\",ALL_CATEGORIES)}.csv'), sep=';')\n",
    "    column_names = themes_for_documents_df.columns.values.tolist()\n",
    "    si_distribution_dict = {}\n",
    "    \n",
    "    for doc_name, row in themes_for_documents_df.iterrows():\n",
    "        si_distribution_dict[doc_name] = dict(zip(column_names, row[column_names]))\n",
    "\n",
    "    with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(subject_name+\"_distribution\",ALL_CATEGORIES)}.json', 'w', encoding='utf8') as fp:\n",
    "        json.dump(si_distribution_dict, fp, ensure_ascii=False)\n",
    "\n",
    "    preprocessed_si_doc_dict = {}\n",
    "    preprocessed_si_doc_dict[subject_name] = subject_content\n",
    "    with open(f'{os.getcwd()}/results/visual/{name_by_categories_no(\"preprocessed_\"+subject_name+\"_doc\",ALL_CATEGORIES)}.json', 'w', encoding='utf8') as fp:\n",
    "        json.dump(preprocessed_si_doc_dict, fp, ensure_ascii=False)\n",
    "    print(f'\\n {subject_name} distribution exported and length of {subject_name} is: {len(subject_content)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4402c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Execution of the LDA solution part 2\n",
    "In this section the exported results will be processed in order to\n",
    "be exported to files that can be viewed and evaluated visually by the dashboards\n",
    "in the results/visual folder.'''\n",
    "    \n",
    "''' This is the execution process'''\n",
    "\n",
    "p_theme_given_topic_df = pd.read_csv(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(\"p_themes_given_topics\",ALL_CATEGORIES)}.csv'), sep=\";\", index_col=0)\n",
    "export_p_theme_given_topic_to_json(p_theme_given_topic_df)\n",
    "    \n",
    "standarize_document_names(\"preprocessed_all_docs\")\n",
    "standarize_document_names(\"preprocessed_test_docs\")\n",
    "\n",
    "\n",
    "print('themes given topics exported.')\n",
    "themes_distribution_over_docs_df = pd.read_csv(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(\"test_docs_distributions\",ALL_CATEGORIES)}.csv'), sep=\";\", index_col=0)\n",
    "themes_distribution_over_docs_df.index = themes_distribution_over_docs_df.index.str.replace(',', '.')\n",
    "export_p_theme_given_documents_to_json(themes_distribution_over_docs_df, \"test_docs_distributions\")\n",
    "print('themes given test documents exported.')    \n",
    "themes_distribution_over_all_docs_df = pd.read_csv(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(\"all_docs_distributions\",ALL_CATEGORIES)}.csv'), sep=\";\", index_col=0)\n",
    "themes_distribution_over_all_docs_df.index = themes_distribution_over_all_docs_df.index.str.replace(',', '.')\n",
    "\n",
    "print('loaded all docs distributions from all datasets')\n",
    "\n",
    "export_p_theme_given_documents_to_json(themes_distribution_over_all_docs_df, f'{name_by_categories_no(\"all_docs_distributions\",ALL_CATEGORIES)}')\n",
    "print('themes given all documents exported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f123caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_closest_documents(themes_distribution_over_docs_df, themes_distribution_over_all_docs_df, 3, f'{os.getcwd()}/results/visual/{name_by_categories_no(\"preprocessed_all_docs\",ALL_CATEGORIES)}.json' , \"closest_documents_distances_from_springer\")\n",
    "print('closest 3 documents exported.')\n",
    "export_closest_2_documents(themes_distribution_over_docs_df, themes_distribution_over_all_docs_df)\n",
    "print('closest 2 documents exported.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faaada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_with_words = pd.read_csv(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(\"topics_with_words\",ALL_CATEGORIES)}.csv'), sep=\";\", index_col=0)\n",
    "dictionary = gensim.corpora.dictionary.Dictionary.load(springer_dictionary_path)\n",
    "lda_model = LdaMallet.load(springer_model_path)\n",
    "p_theme_given_topic_df = pd.read_csv(os.path.join(os.path.abspath(''), f'{os.getcwd()}/results/{name_by_categories_no(\"p_themes_given_topics\",ALL_CATEGORIES)}.csv'), sep=\";\", index_col=0)\n",
    "    \n",
    "export_mixed_documents(lda_model, dictionary, topics_with_words, p_theme_given_topic_df)\n",
    "export_mixed_document_with_closest_documents(lda_model, dictionary, topics_with_words, p_theme_given_topic_df, themes_distribution_over_docs_df, themes_distribution_over_all_docs_df)\n",
    "print('mixed documents exported.')\n",
    "print ('Multisubject LDA on Springer is completed.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
